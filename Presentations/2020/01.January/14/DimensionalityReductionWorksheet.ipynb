{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Dimensionality Reduction?\n",
    "\n",
    "As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. One of the most common ways of doing visualization is through charts. For instance, the relationship between two variables can be easily visualised through a plot.\n",
    "\n",
    "For a case in which we have, say 100 variables (*p*=100), we can have 100(100-1)/2 = 5000 different plots. In such cases where we have a large number of variables, it is better to select a subset of these variables (*p*<<100) which captures as much information as the original set of variables.\n",
    "\n",
    "For instance, we have weights of similar objects in Kg (X_1) and Pound (X_2). If we use both of these variables, they will convey similar information. So, it would make sense to use only one variable, i.e., we can convert the data from 2D to 1D.\n",
    "\n",
    "Reducing *p* dimensions of the data into a subset of *k* dimensions (*k<<n*). This is called dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why is Dimensionality Reduction required?\n",
    "\n",
    "- Space required to store the data is reduced as the number of dimensions comes down.\n",
    "\n",
    "- Less dimensions lead to less computation/training time.\n",
    "\n",
    "- Some algorithms do not perform well for a large number of dimensions, hence reducing these dimensions needs to happen for these algorithms to be useful.\n",
    "\n",
    "- It takes care of multicollinearity by removing redundant features. For example, two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’ are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require.\n",
    "\n",
    "- It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Common Dimensionality Reduction Techniques\n",
    "\n",
    "- Keeping only the most relevant variables from the original dataset (feature selection)\n",
    "\n",
    "- Finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables (dimensionality reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The 2013 sales data for 1559 products across 10 stores of a shop chain in different cities. The data may have missing values as some stores might not report all the data due to technical glitches.\n",
    "\n",
    "**Dimension:** 8523 items x 12 features for each item\n",
    "\n",
    "**Variable - Description**\n",
    "\n",
    "Item_Identifier - Unique product ID\n",
    "\n",
    "Item_Weight - Weight of product\n",
    "\n",
    "Item_Fat_Content - Whether the product is low fat or not\n",
    "\n",
    "Item_Visibility - The percentage of total display area of all products in a store allocated to the particular product\n",
    "\n",
    "Item_Type - The category to which the product belongs\n",
    "\n",
    "Item_MRP - Maximum Retail Price (list price) of the product\n",
    "\n",
    "Outlet_Identifier - Unique store ID\n",
    "\n",
    "Outlet_Establishment_Year - The year in which store was established\n",
    "\n",
    "Outlet_Size - The size of the store in terms of ground area covered\n",
    "\n",
    "Outlet_Location_Type - The type of city in which the store is located\n",
    "\n",
    "Outlet_Type - Whether the outlet is just a grocery store or some sort of supermarket\n",
    "\n",
    "Item_Outlet_Sales - Sales of the product in the particular store. [This is the outcome variable to be predicted.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:15:48.179940Z",
     "start_time": "2020-01-12T17:15:47.492901Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:16:03.528818Z",
     "start_time": "2020-01-12T17:16:03.495816Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Missing Value Ratio\n",
    "\n",
    "For a given dataset, the first naturally step is exploring the data. If the dataset has some missing values, it is recommended to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
    "\n",
    "If too many values are missing (more than 50%, for instance), should the missing values be inputed or should the variable be dropped? In many cases, dropping the variable is recommendable, since it will not infer much information about the data. To be more precise, a threshold value can be set and if the percentage of missing values in any variable is greater than the threshold, the variable is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:16:59.086793Z",
     "start_time": "2020-01-12T17:16:59.080793Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, check the percentage of missing values in each variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the dataset, a percentage threshold can be established and variables having more than that percentage of missing values can be droppped.\n",
    "\n",
    "We will work with a threshold of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:18:28.833926Z",
     "start_time": "2020-01-12T17:18:28.826926Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the number of missing values in a list of variable\n",
    "\n",
    "# drop columns having more than 20% missing values variables list\n",
    "\n",
    "# check the remaining variables list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:18:51.944248Z",
     "start_time": "2020-01-12T17:18:51.922247Z"
    }
   },
   "source": [
    "#### 3.2 Low Variance Filter\n",
    "\n",
    "Consider a variable where all the observations have the same value, say 1. Using this variable has no benefit on the model, since the variance is null.\n",
    "\n",
    "Hence, it is a good idea to drop the variables having low variance as compared to other variables in the dataset. Variables with a low variance will not affect the target variable.\n",
    "\n",
    "**First, fill the missing values**; available options for numerical data: *mean, median, mode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:20:24.649550Z",
     "start_time": "2020-01-12T17:20:24.637550Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill missing values in corresponding columns using the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:20:54.553261Z",
     "start_time": "2020-01-12T17:20:54.548260Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if all missing values have been filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:21:05.419882Z",
     "start_time": "2020-01-12T17:21:05.406882Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the variance of numerical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:25:04.943737Z",
     "start_time": "2020-01-12T17:25:04.939737Z"
    }
   },
   "outputs": [],
   "source": [
    "# consider only columns having numerical values\n",
    "\n",
    "# saving the variables having a variance greater than a given threshold, i.e. 10\n",
    "\n",
    "# drop the variables having small variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 High Correlation Filter\n",
    "\n",
    "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance).\n",
    "\n",
    "We compute the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
    "\n",
    "**General guideline**: keep those variables having a decent or high correlation with the target variable.\n",
    "\n",
    "If we need to train a model for estimating one variable (called *dependent variable*, e.g. Item_Outlet_Sales), we should first eliminate it from the training dataset and save the remaining variables in a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T17:26:00.109893Z",
     "start_time": "2020-01-12T17:26:00.105892Z"
    }
   },
   "outputs": [],
   "source": [
    "# eliminate the dependent variable and compute the correlation table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *high correlation* between a pair of variables (features) is usually considered to be greater than 0.5-0.6 as an absolute value. Note that the considered dataset has no high correlation variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Random Forest\n",
    "\n",
    "Random Forest is one of the most widely used algorithms for feature selection. It helps select a smaller subset of features.\n",
    "\n",
    "The data must be converted into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs.\n",
    "\n",
    "The ID variables (Item_Identifier and Outlet_Identifier) should be dropped as these are just unique numbers and hold no significant importance. Also, the dependent variable (Item_Outlet_Sales) should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "data_rf = data1.drop(['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'], axis = 1) \n",
    "# data1 is the dataset having no missing values\n",
    "model = RandomForestRegressor(random_state = 1, max_depth = 10)\n",
    "data_rf = pd.get_dummies(data_rf)\n",
    "model.fit(data_rf,data1.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_rf.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Backward Feature Elimination\n",
    "\n",
    "- Take all the *n* variables present in the dataset and train the model using them\n",
    "- Compute the performance of the model\n",
    "- Compute the performance of the model after eliminating each variable (*n* times), i.e., dropping one variable every time and training the model on the remaining *n-1* variables\n",
    "- Identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "- Repeat this process until no variable can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# separate the target and independent variable\n",
    "# data_n = numerical columns dataset\n",
    "X = data_n.drop(columns = ['Item_Outlet_Sales'], axis=1)\n",
    "Y = data_n['Item_Outlet_Sales']\n",
    "\n",
    "# create the object of the model\n",
    "lreg = LinearRegression()\n",
    "\n",
    "# specify the number of  features to select (e.g., 3)\n",
    "rfe = RFE(lreg, 3)\n",
    "\n",
    "# fit the model\n",
    "rfe = rfe.fit(X, Y)\n",
    "\n",
    "print('FEATURES SELECTED')\n",
    "print(rfe.support_)\n",
    "\n",
    "print('RANKING OF FEATURES')\n",
    "print(rfe.ranking_)\n",
    "\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Forward Feature Selection\n",
    "\n",
    "- Start with a single feature. Essentially, the model is trained *n* number of times using each feature separately\n",
    "- The variable giving the best performance is selected as the starting variable\n",
    "- Repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained\n",
    "- Repeat this process until no significant improvement is seen in the model’s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "#df = pd.read_csv('train.csv')\n",
    "#df = df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "ffs = f_regression(data_n,data.Item_Outlet_Sales)\n",
    "variable = [ ]\n",
    "for i in range(0,len(data_n.columns)-1):\n",
    "    if ffs[0][i] >=10:\n",
    "       variable.append(data_n.columns[i])\n",
    "variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
