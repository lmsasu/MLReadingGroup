{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad \n",
    "Presented during ML reading group, 2019-11-5.\n",
    "\n",
    "Author: Ioana Plajer, ioana.plajer@unitbv.ro\n",
    "    \n",
    "Reviewed: Lucian Sasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T14:56:57.333835Z",
     "start_time": "2019-11-02T14:56:56.874508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.16.4\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "print(f'Numpy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AdaGrad paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) comes with the idea of using different learning rates for each feature. Hence, instead of:\n",
    "$$w_{t+1} = w_{t} - \\eta \\nabla J_{w}(w_t)$$\n",
    "AdaGrad comes with:\n",
    "$$w_{t+1}^{(j)} = w_{t}^{(j)} - \\frac{\\eta}{\\sqrt{\\varepsilon + \\sum_{\\tau=1}^{t}{(g_{\\tau}^{(j)})^2}}} \\nabla J_{w}(w_t^{(j)})$$\n",
    "where $g_{\\tau}$ is the gradient of error function at iteration $\\tau$, $g_{\\tau}^{(j)}$ is the partial derivative of the \n",
    "error function in direction of the $j$ - th feature, at iteration $\\tau$, $m$ - is the number of features, i.e. \n",
    "$$g_{\\tau}^{(j)} = \\nabla J_{w}(w_\\tau^{(j)})$$\n",
    "\n",
    "AdaGrad specifies the update as:\n",
    "$$w_{t+1} = w_{t} - \\frac{\\eta}{\\sqrt{\\varepsilon I + diag(G_t)}} \\nabla J_{w}(w_t)$$\n",
    "where:\n",
    "* $\\eta$ is the initial learning rate (hyperparameter)\n",
    "* $n$ is the number of items in (mini)batch\n",
    "* $G_t = \\sum\\limits_{\\tau=1}^t \\mathbf{g}_\\tau \\mathbf{g}_\\tau^T$\n",
    "* $diag(A)$ is the diagonal form of the square matrix $A$\n",
    "* $\\varepsilon > 0$ is used to avoid division by 0\n",
    "* $I$ is the unit matrix of size $m$\n",
    "* $G_t^{(j,j)} = \\sum\\limits_{\\tau = 1}^{t}{(g_\\tau^{(j)})^2}$ is the sum of the squared partial derivatives in direction \n",
    "of the $j$ - th feature from the first iteration up to the current iteration\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T09:15:11.887524Z",
     "start_time": "2019-11-01T09:15:11.774457Z"
    }
   },
   "source": [
    "In a more detailed form, the update of the weights through AdaGrad is done by:\n",
    "$$\\left[\\begin{array}{c} \n",
    "         w_{t+1}^{(1)}\\\\\n",
    "         w_{t+1}^{(2)}\\\\\n",
    "         \\vdots\\\\\n",
    "         w_{t+1}^{(m)}\n",
    "         \\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "         w_{t}^{(1)}\\\\\n",
    "         w_{t}^{(2)}\\\\\n",
    "         \\vdots\\\\\n",
    "         w_{t}^{(m)}\\end{array}\\right] - \\eta\\left(\\left[\\begin{array}{cccc} \\varepsilon & 0 & \\ldots & 0\\\\\n",
    "                                                   0 & \\varepsilon & \\ldots & 0\\\\\n",
    "                                                   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                                   0 & 0 & \\ldots & 0\\end{array}\\right]+\n",
    "                                                   \\left[\\begin{array}{cccc}\n",
    "                                                   G_{t}^{(1,1)} & 0 & \\ldots & 0\\\\\n",
    "                                                   0 & G_{t}^{(2,2)} & \\ldots & 0\\\\\n",
    "                                                   \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "                                                   0 & 0 & \\ldots & G_{t}^{(m,m)}\\end{array}\\right]\\right)^{-1/2}\n",
    "                                                    \\left[\\begin{array}{c}\n",
    "                                                    g_t^{(1)}\\\\\n",
    "                                                    g_t^{(2)}\\\\\n",
    "                                                    \\vdots\\\\\n",
    "                                                    g_t^{(m)}\\end{array}\\right]$$\n",
    "which simplifies to:\n",
    "$$\\left[\\begin{array}{c} \n",
    "         w_{t+1}^{(1)}\\\\\n",
    "         w_{t+1}^{(2)}\\\\\n",
    "         \\vdots\\\\\n",
    "         w_{t+1}^{(m)}\n",
    "         \\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "         w_{t}^{(1)}\\\\\n",
    "         w_{t}^{(2)}\\\\\n",
    "         \\vdots\\\\\n",
    "         w_{t}^{(m)}\\end{array}\\right] - \\left[\\begin{array}{c}\n",
    "                                               \\frac{\\eta}{\\sqrt{\\varepsilon+G_{t}^{(1,1)}}}g_t^{(1)}\\\\\n",
    "                                               \\frac{\\eta}{\\sqrt{\\varepsilon+G_{t}^{(2,2)}}}g_t^{(2)}\\\\\n",
    "                                               \\vdots\\\\\n",
    "                                               \\frac{\\eta}{\\sqrt{\\varepsilon+G_{t}^{(m,m)}}}g_t^{(m)}\n",
    "                                               \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:22.895525Z",
     "start_time": "2019-11-02T06:10:22.885519Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import random #to generate sparse data\n",
    "\n",
    "np.random.seed(10) # for reproducibility\n",
    "m_data = 100\n",
    "n_data = 4 #number of features of the data\n",
    "_scales = np.array([1,10, 10,1]) # play with these... \n",
    "\n",
    "\n",
    "_parameters = np.array([3, 0.5, 1, 7]) \n",
    "\n",
    "def gen_data(m, n, scales, parameters, add_noise=True):\n",
    "    # Adagrad is designed especially for sparse data.\n",
    "    # produce: X, a 2d tensor with m lines and n columns\n",
    "    # and X[:, k] uniformly distributed in [-scale_k, scale_k] with the first and the last column containing sparse data \n",
    "    #(approx 75% of the elements are 0)\n",
    "    #\n",
    "    # To generate a sparse data matrix with m rows and n columns\n",
    "    # and random values use S = random(m, n, density=0.25).A, where density = density of the data. S will be the \n",
    "    # resulting matrix \n",
    "    # more information at https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.random.html\n",
    "    #\n",
    "    # To obtain X - generate a random matrix with X[:, k] uniformly distributed in [-scale_k, scale_k]\n",
    "    # set X[:, 0] and X[:, -1] to 0 and add matrix S with the sparse data.\n",
    "    #\n",
    "    # let y be X@parameters.T + epsilon, with epsilon ~ N(0, 1); y is a vector with m elements\n",
    "    # parameters - the ideal weights, used to produce output values y\n",
    "    #\n",
    "    \n",
    "   \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:23.720110Z",
     "start_time": "2019-11-02T06:10:23.716107Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = gen_data(m_data, n_data, _scales, _parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define error function, gradient, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:28.323608Z",
     "start_time": "2019-11-02T06:10:28.317604Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_estimate(X, w):\n",
    "    '''Computes the linear regression estimation on the dataset X, using coefficients w\n",
    "    :param X: a 2d tensor with m_data lines and n_data columns\n",
    "    :param w: a 1d tensor with n_data coefficients (no intercept)\n",
    "    :return: a 1d tensor with m_data elements y_hat = w @X.T\n",
    "    '''\n",
    "    \n",
    "    return y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:28.981094Z",
     "start_time": "2019-11-02T06:10:28.977106Z"
    }
   },
   "outputs": [],
   "source": [
    "def J(X, y, w):\n",
    "    \"\"\"Computes the mean squared error of model. See the picture from last week's sheet.\n",
    "    :param X: input values, of shape m_data x n_data\n",
    "    :param y: ground truth, column vector with m_data values\n",
    "    :param w: column with n_data coeffieicnts for the linear form \n",
    "    :return: a scalar value >= 0\n",
    "    :use the same formula as in the exercise from last week\n",
    "    \"\"\"\n",
    "   \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:29.732627Z",
     "start_time": "2019-11-02T06:10:29.727623Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, w):\n",
    "    '''Commputes the gradients to be used for gradient descent. \n",
    "    :param X: 2d tensor with training data\n",
    "    :param y: 1d tensor with y.shape[0] == W.shape[0]\n",
    "    :param w: 1d tensor with current values of the coefficients\n",
    "    :return: gradients to be used for gradient descent. \n",
    "    :use the same formula as in the exercise from last week\n",
    "    '''\n",
    "    \n",
    "    return grad## implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:30.693332Z",
     "start_time": "2019-11-02T06:10:30.685325Z"
    }
   },
   "outputs": [],
   "source": [
    "#The function from last week for comparison\n",
    "def gd_no_momentum(X, y, w_init, eta=1e-1, thresh = 0.001):\n",
    "    '''Iterates with gradient descent algorithm\n",
    "    :param X: 2d tensor with data\n",
    "    :param y: 1d tensor, ground truth \n",
    "    :param w_init: 1d tensor with the X.shape[1] initial coefficients\n",
    "    :param eta: the learning rate hyperparameter\n",
    "    :param thresh: the threshold for the gradient norm (to stop iterations)\n",
    "    :return: the list of succesive errors and the found w* vector \n",
    "    '''\n",
    "    w = w_init\n",
    "    w_err=[]\n",
    "    \n",
    "    while True:\n",
    "        grad = gradient(X, y, w)\n",
    "        err = J(X, y, w)\n",
    "        w_err.append(err)\n",
    "        w = w - eta * grad\n",
    "        \n",
    "        if np.linalg.norm(grad) < thresh:\n",
    "            break;\n",
    "    \n",
    "    return w_err, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:10:45.322476Z",
     "start_time": "2019-11-02T06:10:45.318476Z"
    }
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0, 0, 0, 0])\n",
    "errors, w_best = gd_no_momentum(X, y, w_init, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T05:11:22.802695Z",
     "start_time": "2019-11-02T05:11:22.796693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations were made: 1001\n"
     ]
    }
   ],
   "source": [
    "print(f'How many iterations were made: {len(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:11:00.234572Z",
     "start_time": "2019-11-02T06:11:00.230568Z"
    }
   },
   "outputs": [],
   "source": [
    "w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:11:16.810992Z",
     "start_time": "2019-11-02T06:11:16.806989Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(errors))), errors)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')\n",
    "axes.set_title('Optimization without momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T05:11:33.548205Z",
     "start_time": "2019-11-02T05:11:33.538196Z"
    }
   },
   "outputs": [],
   "source": [
    "#The function from last week for comparison\n",
    "def gd_with_momentum(X, y, w_init, eta=1e-1, gamma = 0.9, thresh = 0.001):\n",
    "    \"\"\"Applies gradient descent with momentum coefficient\n",
    "    :params: as in gd_no_momentum\n",
    "    :param gamma: momentum coefficient\n",
    "    :param thresh: the threshold for the gradient norm (to stop iterations)\n",
    "    :return: the list of succesive errors and the found w* vector \n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    w_err=[]\n",
    "   \n",
    "    delta = np.zeros_like(w)\n",
    "    while True:\n",
    "        grad = gradient(X, y, w)\n",
    "        err = J(X, y, w)\n",
    "        w_err.append(err)\n",
    "        w_new = w + gamma * delta - eta * grad\n",
    "        delta = w_new - w\n",
    "        w = w_new\n",
    "        \n",
    "        if np.linalg.norm(grad) < thresh :\n",
    "            break;\n",
    "    return w_err, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T05:11:37.703186Z",
     "start_time": "2019-11-02T05:11:34.486905Z"
    }
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0, 0, 0, 0])\n",
    "errors_momentum, w_best = gd_with_momentum(X, y, w_init,0.0001, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:11:37.283659Z",
     "start_time": "2019-11-02T06:11:37.278659Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'How many iterations were made: {len(errors_momentum)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:11:43.819574Z",
     "start_time": "2019-11-02T06:11:43.814573Z"
    }
   },
   "outputs": [],
   "source": [
    "w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:11:56.577370Z",
     "start_time": "2019-11-02T06:11:56.572364Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(errors_momentum))), errors_momentum)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')\n",
    "axes.set_title('Optimization with momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply AdaGrad and report resulting $\\eta$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:12:08.721075Z",
     "start_time": "2019-11-02T06:12:08.714049Z"
    }
   },
   "outputs": [],
   "source": [
    "def ada_grad(X, y, w_init, eta_init=1e-1, eps = 0.001,thresh = 0.001):\n",
    "    '''Iterates with gradient descent algorithm\n",
    "    :param X: 2d tensor with data\n",
    "    :param y: 1d tensor, ground truth \n",
    "    :param w_init: 1d tensor with the X.shape[1] initial coefficients\n",
    "    :param eps: the epsilon value from the AdaGrad formula\n",
    "    :param thresh: the threshold for the gradient norm (to stop iterations)\n",
    "    :return: the list of succesive errors w_err, the found w - the estimated feature vector \n",
    "    :and rates the learning rates after the final iteration \n",
    "    '''\n",
    "     \n",
    "   \n",
    "    return w_err, w, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T05:11:48.155355Z",
     "start_time": "2019-11-02T05:11:47.763080Z"
    }
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0,0,0,0])\n",
    "adaGerr, w_ada_best, rates = ada_grad(X, y, w_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:12:16.391288Z",
     "start_time": "2019-11-02T06:12:16.387286Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'How many iterations were made: {len(adaGerr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:12:22.392964Z",
     "start_time": "2019-11-02T06:12:22.389961Z"
    }
   },
   "outputs": [],
   "source": [
    "w_ada_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T06:12:33.579541Z",
     "start_time": "2019-11-02T06:12:33.575554Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(adaGerr))),adaGerr)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')\n",
    "axes.set_title('Optimization with AdaGrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.712px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
