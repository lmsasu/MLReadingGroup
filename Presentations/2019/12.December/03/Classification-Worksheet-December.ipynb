{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "## Logistic Regression and classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks - Main Idea:\n",
    "For a given set of input data $x_1, x_2, \\ldots, x_n$ - estimate a corresponding output. The output value can represent:\n",
    "* a predicted value (ex. price of an estate, coefficients of a function)\n",
    "* a recognised object (Logistic Regession)\n",
    "* a class to which the object described by the input belongs (Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single neuron\n",
    "![alt text](perceptron3.png)\n",
    "\n",
    "in which \n",
    "- $z = w_1x_1 + w_2x2 + w_3x_3 + \\ldots + w_nx_n + b$\n",
    "- the function $g$ represents the activation function.\n",
    "\n",
    "The resulted value $\\hat{y} = g(z)$ estimates the expected value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The activation function:\n",
    "\n",
    "* decides, whether a neuron should be activated or not. \n",
    "* introduces non-linearity into the output of a neuron, making it possible to learn and perform more complex tasks.\n",
    "\n",
    "A neural network without an activation function is essentially just a linear regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "<ol>\n",
    "    <li>The Sigmoid function. Values lie between 0 and 1</li>\n",
    "    <p>$$sigmoid(z) = \\frac{1}{1 + e^{-z}}$$.</p>\n",
    "    <p> The sigmoid is mostly used on the output layer in logistic regression.</p>\n",
    "    <li>The Tangent Hyperbolic function (tanh). Values lie between -1 and 1. Is sed for the hidden layers</li>\n",
    "    <p>$$tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$</p>\n",
    "    <li>The Rectified Linear function (ReLU). Is used for the hidden layers</li>\n",
    "    <p>$$ReLu(z) = \\left\\{\\begin{array}{l,l} z, & z \\geq 0\\\\ 0, & z < 0 \\end{array}\\right.$$</p>\n",
    "    <p>ReLu is less computationally expensive than tanh and sigmoid. At a time only a few neurons are activated making the network \n",
    "            sparse and thus making it efficient and easy for computation.</p>\n",
    "    <li>The Softmax function - is mostly used in classification. For $z = (z_1, z_2, \\ldots, z_C)$,= output of the last\n",
    "        layer, for one tarining example, using the notations $t_i = e^{z_i}$ and $C$ - number of classes:\n",
    "            $$softmax(z)=\\left[\\begin{array}{c}\\frac{t_1}{\\sum\\limits_{i=1}^{C}{t_i}} \\\\\n",
    "                                        \\frac{t_2}{\\sum\\limits_{i=1}^{C}{t_i}}\\\\\n",
    "                                        \\ldots\\\\\n",
    "                                        \\frac{t_C}{\\sum\\limits_{i=1}^{C}{t_i}}\\end{array}\\right]$$</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function: \n",
    "\n",
    "Usually the _cross-entopy_ function is used as loss function for $y^{(i)}$,the expected output for the $i$ - th example in the training set.\n",
    "\n",
    "If we consider $a^{(i)} = y^{(i)}$ - the output of the last layer for the $i$-th training example:\n",
    "\n",
    "<ol>\n",
    "    <li> For logistic regression - with a single neuron on the output layer:</li>\n",
    "    <p>$$J(y^{(i)},a^{(i)}) = -\\left(y^{(i)}\\log{a^{(i)}} + (1 - y^{(i)})\\log{(1 - a^{(i)})} \\right)$$</p>\n",
    "    <p>The derivative of $J(a^{(i)},y^{(i)})$ with respect to $a^{(i)}$ is:</p>\n",
    "    <p>$$\\nabla J_{a}(a^{(i)}, y^{(i)}) = -\\frac{y^{(i)}}{a^{(i)}} + \\frac{(1 - y^{(i)})}{(1 - a^{(i)})}$$</p>\n",
    "    <p> </p>\n",
    "    <li> For classification with $C$ classes:  </li>\n",
    "    <p>$$J(y^{(i)},a^{(i)}) = -\\sum\\limits_{j=1}^{C}{y^{(i)}_j\\log{a^{(i)}_j}}$$ </p>\n",
    "    <p>The derivative of $J(a^{(i)},y^{(i)})$ with respect to $a^{(i)}$ is:</p>\n",
    "    <p>$$\\nabla J_{a}(y^{(i)},a^{(i)}) = -\\frac{y^{(i)}}{a^{(i)}}$$</p>\n",
    "</ol>\n",
    "\n",
    "The error function is the mean of losses for all the $m$ training examples. Vectorized thet can be written:\n",
    "<ol>\n",
    "    <li> For logistic regression</li>\n",
    "    <p>$$J:= -\\frac{1}{m}*\\left(Y*np.log{(A)} + (1 - Y)*np.log{(1 - A)} \\right)$$</p>\n",
    "    <li> For classification with $C$ classes:  </li>\n",
    "    <ol>\n",
    "        <li> for one training example, the output $\\hat{y}^{(i)} = (\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)},\\ldots, \\hat{y}_C^{(i)})$</li>\n",
    "        <p>$$loss(y^{(i)},\\hat{y}^{(i)}) = -\\sum\\limits_{k=1}^{C}{y^{(i)}\\log{\\hat{y}^{(i)}}}$$</p>\n",
    "        <p>which can be written in numpy vectorized as:</p> \n",
    "        <p>$$loss = - np.sum(Y*np.log{(A)}, axis = 1)$$</p>\n",
    "        <li> for the (mini-)batch containing all the $m$ examples:</li>  \n",
    "        <p>$$J(Y,\\hat{Y}) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}{loss(y^{(i)},\\hat{y}^{(i)})}$$</p>\n",
    "        <p>which can be written in numpy vectorized as:</p>\n",
    "        <p>$$J:= -\\frac{1}{m}*np.sum(loss)$$ </p>\n",
    "   </ol>     \n",
    "</ol>\n",
    "$m$ = number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Calculation\n",
    "![alt text](BackwardCalc2.png)\n",
    "\n",
    "Considering a single neuron, with sigmoid activation function, the weight updates  are calculated as follows:\n",
    "<ol>\n",
    "    <li> Derivative calculation:\n",
    "    $$\\nabla J_{a} = \\frac{\\partial J}{\\partial a} = -\\frac{y}{a} + \\frac{1 - y}{1 - a}$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{z} = \\frac{\\partial J}{\\partial z} = \\frac{\\partial J}{\\partial a}\\frac{\\partial a}{\\partial z} = a - y$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{w_i} = \\frac{\\partial J}{\\partial w_i} = \\frac{\\partial J}{\\partial z}\\frac{\\partial z}{\\partial w_i} =\n",
    "    x_i\\frac{\\partial J}{\\partial z}$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{b} = \\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial z}\\frac{\\partial z}{\\partial b} = \n",
    "    \\frac{\\partial J}{\\partial z}$$\n",
    "    </li>\n",
    "    <li> Weights update:\n",
    "    $$ w_i = w_i - \\eta\\nabla J_{w_i}$$\n",
    "    <p> </p>\n",
    "    $$ b = b - \\eta\\nabla J_{b}$$\n",
    "    where $\\eta$ is the learning rate.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with a single hidden layer\n",
    "![alt text](NN2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations:\n",
    "$[nr]$ - number of the layer, $[0]$ = input layer\n",
    "\n",
    "$n^{[i]}$ - number of neurons in the $i$-th layer\n",
    "\n",
    "$z^{[i]}_j$ - linear function for the $j$-th neuron in the $i$-th layer, $Z^{[i]}$ - vectorized notation for layer $i$\n",
    "\n",
    "$a^{[i]}_j = g^{[i]}(z^{[i]}_j) $ - activation of the $j$-th neuron in the $i$-th layer, $A^{[i]}$ - vectorized notation for layer $i$\n",
    "\n",
    "$W^{[i]}$ - weights matrix of dimensions $n^{[i]}\\times n^{[i-1]}$ for the layer $i$\n",
    "\n",
    "$J$ - error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One iteration through the network\n",
    " Neural Network with a single hidden layer\n",
    "![alt text](BackProp2.png)\n",
    "\n",
    "The blue lines represent the forward propagation\n",
    "\n",
    "The red lines represent the backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations:\n",
    "\n",
    "$dz^{[i]} := \\frac{\\partial J}{\\partial z^{[i]}}$ - derivative of the error function in $z^{[i]}$ direction\n",
    "\n",
    "$da^{[i]} := \\frac{\\partial J}{\\partial a^{[i]}}$ - derivative of the error function in $z^{[i]}$ direction\n",
    "\n",
    "$dW^{[i]} := \\frac{\\partial J}{\\partial W^{[i]}}$ - derivative of the error function in $W^{[i]}$ direction\n",
    "\n",
    "$db^{[i]} := \\frac{\\partial J}{\\partial b^{[i]}}$ - derivative of the error function in $b^{[i]}$ direction\n",
    "\n",
    "$X$ - the input $m \\times n^{[0]}$ matrix, where each ROW reprezents a training example\n",
    "\n",
    "$m$ - the number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas for the Forward Propagation\n",
    "\n",
    "### First layer:\n",
    "\n",
    "$Z^{[1]} = X(W^{[1]})^T + b^{[1]}$\n",
    "\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "\n",
    "### Output (second) layer:\n",
    "\n",
    "$Z^{[2]} = A^{[1]}(W^{[2]})^T + b^{[2]}$\n",
    "\n",
    "$\\hat{y} = A^{[2]} = g^{[2]}(Z^{[2]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T14:02:01.344873Z",
     "start_time": "2019-11-30T14:02:01.322856Z"
    }
   },
   "source": [
    "## Formulas for the Backward Propagation (vectorized)\n",
    "\n",
    "### Output (second) layer:\n",
    "\n",
    "$dA^{[2]} = \\nabla J_{A^{2}}(A^{[2]}, y) = -\\frac{y}{A^{[2]}}$ - for the softmax activation\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - y$\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m}(dZ^{[2]})^T\\left(A^{[1]}\\right)$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis = 0, keepdims = True) $\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[2]}:= W^{[2]} - \\eta dW^{[2]}$\n",
    "    \n",
    "$b^{[2]}:= b^{[2]} - \\eta db^{[2]}$\n",
    "\n",
    "### First layer:\n",
    "\n",
    "$dA^{[1]} = dZ^{[2]}W^{[2]}$\n",
    "\n",
    "$dZ^{[1]} = dA^{[1]}g^{[1]}\\_deriv(Z^{[1]})$\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}(dZ^{[1]})^T\\left(A^{[0]}\\right)^T$\n",
    "\n",
    "$db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis = 0, keepdims = True) $\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[1]}:= W^{[1]} - \\eta dW^{[1]}$\n",
    "    \n",
    "$b^{[1]}:= b^{[1]} - \\eta db^{[1]}$\n",
    "\n",
    "$A^{[0]} = X$.\n",
    "\n",
    "$g^{[1]}\\_deriv$ - derivative of activation function $g$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T13:10:27.557695Z",
     "start_time": "2019-11-21T13:10:27.547706Z"
    }
   },
   "source": [
    "## Generalization for L layers (L - 1 hidden layers)\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "In a for - loop over the layers, in which $l$ is the current layer:\n",
    "    \n",
    "$Z^{[l]} = A^{[l-1]}(W^{[l]})^T + b^{[l]}$\n",
    "\n",
    "$A^{[l]} = g^{[l]}(Z^{[l]})$\n",
    "\n",
    "$g^{[l]}$ - activation function in layer $l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T13:10:05.485502Z",
     "start_time": "2019-11-21T13:10:05.470495Z"
    }
   },
   "source": [
    "### Backward propagation\n",
    "\n",
    "For the output layer $L$, at which backpropagation starts:\n",
    "\n",
    "$dA^{[L]} = -\\frac{y}{A^{[L]}}$\n",
    "\n",
    "as the activation function of the output layer is the softmax and the number of neurons in the layer is $C$ - the number of classes considered.\n",
    "\n",
    "The back propagation is performed in a for - loop over the layers, starting with the output layer, down to the first layer, in which $l$ is the current layer:\n",
    "\n",
    "\n",
    "$dZ^{[l]} = dA^{[l]}g^{[l]}\\_deriv(Z^{[l]})$\n",
    "\n",
    "$dW^{[l]} = \\frac{1}{m}dZ^{[l]}\\left(A^{[l - 1]}\\right)^T$\n",
    "\n",
    "$db^{[l]} = \\frac{1}{m}np.sum(dZ^{[l]}, axis = 1, keepdims = True) $\n",
    "\n",
    "$dA^{[l - 1]} = (W^{[l]})^TdZ^{[l]}$\n",
    "\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[l]}:= W^{[l]} - \\eta dW^{[l]}$\n",
    "    \n",
    "$b^{[l]}:= b^{[l]} - \\eta db^{[l]}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T11:59:56.406843Z",
     "start_time": "2019-12-03T11:59:55.375066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.16.4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T11:59:56.626955Z",
     "start_time": "2019-12-03T11:59:56.431818Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './_semeion/semeion.data'\n",
    "df = pd.read_csv(path, sep=' ', header=None)\n",
    "all_values = df.values[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T11:59:58.588379Z",
     "start_time": "2019-12-03T11:59:58.577375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, 266)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T11:59:59.765215Z",
     "start_time": "2019-12-03T11:59:59.760211Z"
    }
   },
   "outputs": [],
   "source": [
    "#set up the input examples and the corresponding classes\n",
    "X_semeion = all_values[:, :-10]\n",
    "y_semeion = all_values[:, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:00:00.584819Z",
     "start_time": "2019-12-03T12:00:00.578814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1593)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_semeion.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T16:12:18.655502Z",
     "start_time": "2019-11-26T16:12:18.646495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_semeion[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T16:14:16.650700Z",
     "start_time": "2019-11-26T16:14:16.638676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_semeion_pos_max = np.argmax(y_semeion, axis=1)\n",
    "y_semeion_pos_max[1400:1403]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:00:10.112079Z",
     "start_time": "2019-12-03T12:00:10.105078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n"
     ]
    }
   ],
   "source": [
    "#define the training set\n",
    "m = X_semeion.shape[0]\n",
    "print(m)\n",
    "m_train = 2*m//3\n",
    "X_semeion_train = X_semeion[:m_train,:]\n",
    "y_semeion_train = y_semeion[:m_train,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:28.022449Z",
     "start_time": "2019-12-03T12:01:28.015445Z"
    }
   },
   "outputs": [],
   "source": [
    "#initialization of the parameters\n",
    "np.random.seed(10) \n",
    "def initialization(dim_layers):\n",
    "    #dim_layers - array containing the dimensions of each layer\n",
    "    #dim_layers[0] - dimension of an input example\n",
    "    #dim_layers[L-1] - dimension of the output = number of classes\n",
    "    #L - the number layers\n",
    "    #parameters - dictionary for the parameters W and b\n",
    "    #the W[l] - parameters should be set as random values with normal distribution N(0, 1), multiplied with 0.01 \n",
    "    #the size of W[l] is (dim_layers[l] x dim_layers[l-1])\n",
    "    #the b[l] - parameters are initially arrays of 0 with dimensions (1 x dim_layers[l]) \n",
    "    #l - index of the layer\n",
    "    \n",
    "    L = len(dim_layers)\n",
    "    parameters ={}\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = None #initialize values\n",
    "        parameters[\"b\" + str(l)] = None #initialize values\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:34.279497Z",
     "start_time": "2019-12-03T12:01:34.274497Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:34.919983Z",
     "start_time": "2019-12-03T12:01:34.914980Z"
    }
   },
   "outputs": [],
   "source": [
    "def ReLu(z):\n",
    "    rez = z.copy()\n",
    "    rez[rez<0] = 0\n",
    "    return rez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:35.562442Z",
     "start_time": "2019-12-03T12:01:35.557435Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    rez = np.exp(z)\n",
    "    suma = np.sum(rez, axis = 1, keepdims = True)\n",
    "    \n",
    "    rez = rez/suma\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:36.204936Z",
     "start_time": "2019-12-03T12:01:36.199933Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    rez = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:37.063573Z",
     "start_time": "2019-12-03T12:01:37.059585Z"
    }
   },
   "outputs": [],
   "source": [
    "def ReLuBack(rlu):\n",
    "    grad = rlu.copy()\n",
    "    grad[grad>0] = 1\n",
    "    grad[grad<=0]= 0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:37.940232Z",
     "start_time": "2019-12-03T12:01:37.937230Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanhBack(tan):\n",
    "    grad = (1 - tan)**2\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:38.779874Z",
     "start_time": "2019-12-03T12:01:38.775869Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoidBack(sigm):\n",
    "    grad = sigm*(1-sigm)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:39.898691Z",
     "start_time": "2019-12-03T12:01:39.892687Z"
    }
   },
   "outputs": [],
   "source": [
    "#Estimation of the linear model Z in the current layer with calculation of the activation of Z\n",
    "\n",
    "def model_estimate(A_prev, W, b, activation):\n",
    "    #A_prev = activation of the precedent layer\n",
    "    #W, b - parameters for the current layer\n",
    "    #activation  - activation function for the current layer\n",
    "    #return value Z - linear model Z = A_prew@W.T + b\n",
    "    #return value A - activation of the current layer\n",
    "    \n",
    "    Z = None #Calculate Z using the forward propagation general formula    \n",
    "    \n",
    "    if activation ==\"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"ReLu\":\n",
    "        A = ReLu(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    else:\n",
    "        A = softmax(Z)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:41.449792Z",
     "start_time": "2019-12-03T12:01:41.441801Z"
    }
   },
   "outputs": [],
   "source": [
    "#One pass of the forwar propagation\n",
    "\n",
    "def forward_prop(X, y, parameters, activations, number_of_layers):\n",
    "    #number_of_layers - number of layers in the model\n",
    "    #cache - a dictionary containing the linear results and the activations in each layer,\n",
    "    #obtained during forward propagation\n",
    "    #X - input data of size (m x X.shape[1]), m = X.shape[0] = number of the training examples\n",
    "    #y - ideal output z size (m x C), C = number of classes\n",
    "    #parameters - current parameters of the NN = dictionary containing the values for the parameters in each layer\n",
    "    \n",
    "    #return value AL - activation of the last layer = current output of the NN, of size (m x C)\n",
    "    #return value cache - the cache containing the current linear models and activations on each layer\n",
    "    \n",
    "    #to obtain the parameters on layer l, use the values parameters[\"W\"+str(l)], parameters[\"b\"+str(l)]\n",
    "    \n",
    "    L = number_of_layers\n",
    "    cache = {}\n",
    "    A_prev = X\n",
    "    activation = activations[L-1]\n",
    "    cache[\"A0\"] = X \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        W = None #extract parameter W[l] from dictionary\n",
    "        b = None #extract parameter b[l] from dictionary\n",
    "        Z, A = None #calculate the linear model and the activation on the current layer l \n",
    "        #set the cache values for A[l] and Z[l]\n",
    "        A_prev = None\n",
    "    \n",
    "    \n",
    "    AL = None #AL - activation on the last layer\n",
    "    return AL, cache \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:44.301938Z",
     "start_time": "2019-12-03T12:01:44.294927Z"
    }
   },
   "outputs": [],
   "source": [
    "#error function = cross-entropy\n",
    "def error(X, y, AL):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    loss = None #use loss cross-entropy formula\n",
    "    J = None #use error function as a mean of the losses\n",
    "        \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:45.388728Z",
     "start_time": "2019-12-03T12:01:45.380725Z"
    }
   },
   "outputs": [],
   "source": [
    "#derivative of the activation in the current layer, as function of the activation, for sigmoid, ReLu and tanh\n",
    " \n",
    "def deriv(activation, z):\n",
    "    if activation == \"sigmoid\":\n",
    "        rez = sigmoidBack(z)\n",
    "    elif activation == \"ReLu\":\n",
    "        rez = ReLuBack(z)\n",
    "    else: \n",
    "        rez = tanhBack(z)\n",
    "    \n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:46.474552Z",
     "start_time": "2019-12-03T12:01:46.468548Z"
    }
   },
   "outputs": [],
   "source": [
    "#function which calculates the derivative of the error with respect to the linear model Z in the current level,\n",
    "#as function of the derivative of the activation in the previous level, the activation in the current level\n",
    "# and the activation function\n",
    "\n",
    "def calculate_dz(dA, A, y, activation):\n",
    "    if activation == \"softmax\":\n",
    "        rez = A - y\n",
    "    else:\n",
    "        rez = dA*deriv(activation, A)\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:52.398115Z",
     "start_time": "2019-12-03T12:01:52.381104Z"
    }
   },
   "outputs": [],
   "source": [
    "#one iteration of back propagation\n",
    "def back_prop(y, cache, activations, parameters, number_of_layers, learning_rate):\n",
    "    #y - the ideal output\n",
    "    #cache - the cache obtained in the forward prop, containind the activations and the linear models in each layer\n",
    "    #activations - array containing the activation function names for each layer\n",
    "    #activations[0] = \" \". There is no activation on the input layer\n",
    "    #parameters - a dictionary contaning the actual values of the parameters W and b in each layer\n",
    "    #\n",
    "    \n",
    "    #return value - actual_parameters = actualized parameters W, b\n",
    "    \n",
    "    L = number_of_layers - 1 #index of the last layer\n",
    "    m = None #number of training examples\n",
    "    \n",
    "    AL = cache[\"A\"+str(L)] #activation on the last layer\n",
    "    activation = activations[L] #activation function of the output layer = softmax \n",
    "    \n",
    "    dA = -y/AL #derivative of the error with respect to the activation\n",
    "\n",
    "    actual_param = parameters.copy() #copy of the input parameters\n",
    "    A = AL #current activation\n",
    "    \n",
    "    \n",
    "    for l in range(L,0, -1):\n",
    "        dZ = None #calculate derivative using the provided function calculate_dz \n",
    "        A_prev = None #extract the value of the activation on layer l - 1 from the cache dictionary\n",
    "        \n",
    "        Z = None #extract the Z[l] value on layer l from the cache dictionary\n",
    "        \n",
    "        dW = None #use provided formula\n",
    "        db = None #use provided formula\n",
    "                        \n",
    "        activation = \"\" #get activation function of the current layer from the activations vector\n",
    "        W = None #get the current W[l] parameter from the parameters dictionary\n",
    "        \n",
    "        dA = None #calculate using provided formula\n",
    "        A = A_prev\n",
    "        \n",
    "            \n",
    "        actual_param[\"W\" + str(l)] = None #adjust weights W[l]:=W[l] - learning_rate*dw\n",
    "        actual_param[\"b\" + str(l)] = None #adjust weights\n",
    "        \n",
    "       \n",
    "    return actual_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:01:53.694036Z",
     "start_time": "2019-12-03T12:01:53.686029Z"
    }
   },
   "outputs": [],
   "source": [
    "#one iteration thrugh the NN: forward propagation - error calculation - back propagation\n",
    "\n",
    "def one_iteration(X, y, parameters, number_of_layers, activations, learning_rate = 0.01):\n",
    "    #X - training set - batch or minibatch\n",
    "    #y - ideal output for the trainig set\n",
    "    #parameters - the actual parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #learning_rate - learning rate for the update of the parameters\n",
    "    #AL - activation on the last layer - a numpy array of dimensions (m x C), C - number of classes, m - X.shape[0]\n",
    "    #cache - a dictionary containing the activations and linear models on each layer, obtained during forward prop\n",
    "    \n",
    "    #returned value J - error of the current NN - calculated with cross-entropy\n",
    "    #returned value parameters - actualized parameters W, b for each layer, calculated during back prop\n",
    "    \n",
    "    AL, cache = None #apply forward prop\n",
    "    J = None #Calculate error\n",
    "       \n",
    "    parameters = None #apply back_prop\n",
    "    \n",
    "    return J, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:34:55.943402Z",
     "start_time": "2019-12-03T12:34:55.936402Z"
    }
   },
   "outputs": [],
   "source": [
    "#function which iterates through the NN, to minimize the model\n",
    "#output of the function  - the error list and the estimated weights for the model\n",
    "\n",
    "def run_model(X, y, parameters, number_of_layers, activations, learning_rate = 0.1, number_of_iteration = 1000):\n",
    "    #X - training set - batch or minibatch\n",
    "    #y - ideal output for the trainig set\n",
    "    #parameters - the actual parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #learning_rate - learning rate for the update of the parameters    \n",
    "    \n",
    "    #return value w_err - vector of errors at each iteration\n",
    "    #return value param  - estimated parameters\n",
    "    \n",
    "    w_err=[]\n",
    "    param = parameters.copy()\n",
    "    nr = 0\n",
    "    \n",
    "    #apply iteratively the one_iteration function for a number_of_iterations times and append current error th w_err    \n",
    "        \n",
    "    return w_err, param    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:04:55.158356Z",
     "start_time": "2019-12-03T12:04:55.150367Z"
    }
   },
   "outputs": [],
   "source": [
    "#dim_layers - vector containing the size of each layer. Layer 0 - dimension of a training example\n",
    "#activations - vector with the activations on each layer. The input layer has no activation function (identity)\n",
    "\n",
    "input_size = X_semeion_train.shape[1]\n",
    "output_size = y_semeion_train.shape[1]\n",
    "\n",
    "dim_layers = np.array([input_size, 4, output_size ]) #a NN with one hidden layer and 4 neurons\n",
    "activations = np.array([\" \",\"ReLu\", \"softmax\"]) #on the hidden layer we have ReLu activation function\n",
    "\n",
    "parameters = initialization(dim_layers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:35:03.331790Z",
     "start_time": "2019-12-03T12:34:59.621159Z"
    }
   },
   "outputs": [],
   "source": [
    "w_err, param = run_model(X_semeion_train, y_semeion_train, parameters, len(dim_layers), activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_err[len(w_err)-1])\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:35:03.768117Z",
     "start_time": "2019-12-03T12:35:03.503913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(w_err))),w_err)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:29:46.494966Z",
     "start_time": "2019-12-03T12:29:46.488965Z"
    }
   },
   "outputs": [],
   "source": [
    "#predictions for input values X\n",
    "def prediction(X, y, parameters, activations, number_of_layers):\n",
    "    #X - input data \n",
    "    #y - ideal output for the input data\n",
    "    #parameters - estimated parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #return value pred - array containing for each example the position of the maximum probability = corresponding to the class\n",
    "    \n",
    "    AL, _ = forward_prop(X, y, parameters, activations,  number_of_layers)\n",
    "    \n",
    "    pred = np.argmax(AL, axis=1)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:29:58.738334Z",
     "start_time": "2019-12-03T12:29:58.729329Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_prediction(X, y, parameters, activations, number_of_layers):\n",
    "    #X - input data \n",
    "    #y - ideal output for the input data\n",
    "    #parameters - estimated parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    \n",
    "    #return value - percent - percentage of correct predictions\n",
    "    nr = 0\n",
    "   \n",
    "    pred = prediction(X, y, parameters, activations, number_of_layers)\n",
    "    classes = np.argmax(y, axis=1)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        if pred[i] == classes[i]:\n",
    "            nr +=1\n",
    "   \n",
    "    percent = nr/X.shape[0]*100\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:35:09.811653Z",
     "start_time": "2019-12-03T12:35:09.800662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.53860640301318\n"
     ]
    }
   ],
   "source": [
    "#accuracy on the training set\n",
    "number_of_layers = len(dim_layers)\n",
    "rez = test_prediction(X_semeion_train, y_semeion_train, param, activations, number_of_layers)\n",
    "print(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T12:35:12.459828Z",
     "start_time": "2019-12-03T12:35:12.447824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(530, 10)\n",
      "75.84905660377359\n"
     ]
    }
   ],
   "source": [
    "#accuracy on the test set\n",
    "m = X_semeion.shape[0]\n",
    "X_semeion_test = X_semeion[2*m//3+1:,:]\n",
    "y_semeion_test = y_semeion[2*m//3+1:,:]\n",
    "print(y_semeion_test.shape)\n",
    "rez2 = test_prediction(X_semeion_test, y_semeion_test, param, activations, number_of_layers)\n",
    "print(rez2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
