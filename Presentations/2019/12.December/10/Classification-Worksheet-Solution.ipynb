{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "## Logistic Regression and classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks - Main Idea:\n",
    "For a given set of input data $x_1, x_2, \\ldots, x_n$ - estimate a corresponding output. The output value can represent:\n",
    "* a predicted value (ex. price of an estate, coefficients of a function)\n",
    "* a recognised object (Logistic Regession)\n",
    "* a class to which the object described by the input belongs (Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single neuron\n",
    "![alt text](perceptron3.png)\n",
    "\n",
    "in which \n",
    "- $z = w_1x_1 + w_2x2 + w_3x_3 + \\ldots + w_nx_n + b$\n",
    "- the function $g$ represents the activation function.\n",
    "\n",
    "The resulted value $\\hat{y} = g(z)$ estimates the expected value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The activation function:\n",
    "\n",
    "* decides, whether a neuron should be activated or not. \n",
    "* introduces non-linearity into the output of a neuron, making it possible to learn and perform more complex tasks.\n",
    "\n",
    "A neural network without an activation function is essentially just a linear regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "<ol>\n",
    "    <li>The Sigmoid function. Values lie between 0 and 1</li>\n",
    "    <p>$$sigmoid(z) = \\frac{1}{1 + e^{-z}}$$.</p>\n",
    "    <p> The sigmoid is mostly used on the output layer in __logistic regression__.</p>\n",
    "    <li>The Tangent Hyperbolic function (tanh). Values lie between -1 and 1. Is sed for the hidden layers</li>\n",
    "    <p>$$tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$</p>\n",
    "    <li>The Rectified Linear function (ReLU). Is used for the hidden layers</li>\n",
    "    <p>$$ReLu(z) = \\left\\{\\begin{array}{l,l} z, & z \\geq 0\\\\ 0, & z < 0 \\end{array}\\right.$$</p>\n",
    "    <p>ReLu is less computationally expensive than tanh and sigmoid. At a time only a few neurons are activated making the network \n",
    "            sparse and thus making it efficient and easy for computation.</p>\n",
    "    <li>The Softmax function - is mostly used in __classification__. For $z = (z_1, z_2, \\ldots, z_C)$,= output of the last\n",
    "        layer, for one training example, using the notations $t_i = e^{z_i}$ and $C$ - number of classes:\n",
    "            $$softmax(z)=\\left[\\begin{array}{c}\\frac{t_1}{\\sum\\limits_{i=1}^{C}{t_i}} \\\\\n",
    "                                        \\frac{t_2}{\\sum\\limits_{i=1}^{C}{t_i}}\\\\\n",
    "                                        \\ldots\\\\\n",
    "                                        \\frac{t_C}{\\sum\\limits_{i=1}^{C}{t_i}}\\end{array}\\right]$$</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function: \n",
    "\n",
    "Usually the _cross-entopy_ function is used as loss function for $y^{(i)}$,the expected output for the $i$ - th example in the training set.\n",
    "\n",
    "If we consider $a^{(i)} = y^{(i)}$ - the output of the last layer for the $i$-th training example:\n",
    "\n",
    "<ol>\n",
    "    <li> For logistic regression - with a single neuron on the output layer:</li>\n",
    "    <p>$$J(y^{(i)},a^{(i)}) = -\\left(y^{(i)}\\log{a^{(i)}} + (1 - y^{(i)})\\log{(1 - a^{(i)})} \\right)$$</p>\n",
    "    <p>The derivative of $J(a^{(i)},y^{(i)})$ with respect to $a^{(i)}$ is:</p>\n",
    "    <p>$$\\nabla J_{a}(a^{(i)}, y^{(i)}) = -\\frac{y^{(i)}}{a^{(i)}} + \\frac{(1 - y^{(i)})}{(1 - a^{(i)})}$$</p>\n",
    "    <p> </p>\n",
    "    <li> For classification with $C$ classes:  </li>\n",
    "    <p>$$J(y^{(i)},a^{(i)}) = -\\sum\\limits_{j=1}^{C}{y^{(i)}_j\\log{a^{(i)}_j}}$$ </p>\n",
    "    <p>The derivative of $J(a^{(i)},y^{(i)})$ with respect to $a^{(i)}$ is:</p>\n",
    "    <p>$$\\nabla J_{a}(y^{(i)},a^{(i)}) = -\\frac{y^{(i)}}{a^{(i)}}$$</p>\n",
    "</ol>\n",
    "\n",
    "The error function is the mean of losses for all the $m$ training examples. Vectorized thet can be written:\n",
    "<ol>\n",
    "    <li> For logistic regression</li>\n",
    "    <p>$$J:= -\\frac{1}{m}*\\left(Y*np.log{(A)} + (1 - Y)*np.log{(1 - A)} \\right)$$</p>\n",
    "    <li> For classification with $C$ classes:  </li>\n",
    "    <ol>\n",
    "        <li> for one training example, the output $\\hat{y}^{(i)} = (\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)},\\ldots, \\hat{y}_C^{(i)})$</li>\n",
    "        <p>$$loss(y^{(i)},\\hat{y}^{(i)}) = -\\sum\\limits_{k=1}^{C}{y^{(i)}\\log{\\hat{y}^{(i)}}}$$</p>\n",
    "        <p>which can be written in numpy vectorized as:</p> \n",
    "        <p>$$loss = - np.sum(Y*np.log{(A)}, axis = 1)$$</p>\n",
    "        <li> for the (mini-)batch containing all the $m$ examples:</li>  \n",
    "        <p>$$J(Y,\\hat{Y}) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}{loss(y^{(i)},\\hat{y}^{(i)})}$$</p>\n",
    "        <p>which can be written in numpy vectorized as:</p>\n",
    "        <p>$$J:= \\frac{1}{m}*np.sum(loss)$$ </p>\n",
    "   </ol>     \n",
    "</ol>\n",
    "$m$ = number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Calculation\n",
    "![alt text](BackwardCalc3.png)\n",
    "\n",
    "Considering a single neuron, with sigmoid activation function, the weight updates  are calculated as follows:\n",
    "<ol>\n",
    "    <li> Derivative calculation:\n",
    "    $$\\nabla J_{a} = \\frac{\\partial J}{\\partial a} = -\\frac{y}{a} + \\frac{1 - y}{1 - a}$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{z} = \\frac{\\partial J}{\\partial z} = \\frac{\\partial J}{\\partial a}\\frac{\\partial a}{\\partial z} = a - y$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{w_i} = \\frac{\\partial J}{\\partial w_i} = \\frac{\\partial J}{\\partial z}\\frac{\\partial z}{\\partial w_i} =\n",
    "    x_i\\frac{\\partial J}{\\partial z}$$\n",
    "    <p> </p>\n",
    "    $$\\nabla J_{b} = \\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial z}\\frac{\\partial z}{\\partial b} = \n",
    "    \\frac{\\partial J}{\\partial z}$$\n",
    "    </li>\n",
    "    <li> Weights update:\n",
    "    $$ w_i = w_i - \\eta\\nabla J_{w_i}$$\n",
    "    <p> </p>\n",
    "    $$ b = b - \\eta\\nabla J_{b}$$\n",
    "    where $\\eta$ is the learning rate.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with a single hidden layer\n",
    "![alt text](NN2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations:\n",
    "$[nr]$ - number of the layer, $[0]$ = input layer\n",
    "\n",
    "$n^{[i]}$ - number of neurons in the $i$-th layer\n",
    "\n",
    "$z^{[i]}_j$ - linear function for the $j$-th neuron in the $i$-th layer, $Z^{[i]}$ - vectorized notation for layer $i$\n",
    "\n",
    "$a^{[i]}_j = g^{[i]}(z^{[i]}_j) $ - activation of the $j$-th neuron in the $i$-th layer, $A^{[i]}$ - vectorized notation for layer $i$\n",
    "\n",
    "$W^{[i]}$ - weights matrix of dimensions $n^{[i]}\\times n^{[i-1]}$ for the layer $i$\n",
    "\n",
    "$J$ - error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One iteration through the network\n",
    " Neural Network with a single hidden layer\n",
    "![alt text](BackProp2.png)\n",
    "\n",
    "The blue lines represent the forward propagation\n",
    "\n",
    "The red lines represent the backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations:\n",
    "\n",
    "$dz^{[i]} := \\frac{\\partial J}{\\partial z^{[i]}}$ - derivative of the error function in $z^{[i]}$ direction\n",
    "\n",
    "$da^{[i]} := \\frac{\\partial J}{\\partial a^{[i]}}$ - derivative of the error function in $z^{[i]}$ direction\n",
    "\n",
    "$dW^{[i]} := \\frac{\\partial J}{\\partial W^{[i]}}$ - derivative of the error function in $W^{[i]}$ direction\n",
    "\n",
    "$db^{[i]} := \\frac{\\partial J}{\\partial b^{[i]}}$ - derivative of the error function in $b^{[i]}$ direction\n",
    "\n",
    "$X$ - the input $m \\times n^{[0]}$ matrix, where each ROW reprezents a training example\n",
    "\n",
    "$m$ - the number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas for the Forward Propagation\n",
    "\n",
    "### First layer:\n",
    "\n",
    "$Z^{[1]} = X(W^{[1]})^T + b^{[1]}$\n",
    "\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "\n",
    "### Output (second) layer:\n",
    "\n",
    "$Z^{[2]} = A^{[1]}(W^{[2]})^T + b^{[2]}$\n",
    "\n",
    "$\\hat{y} = A^{[2]} = g^{[2]}(Z^{[2]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T14:02:01.344873Z",
     "start_time": "2019-11-30T14:02:01.322856Z"
    }
   },
   "source": [
    "## Formulas for the Backward Propagation (vectorized)\n",
    "\n",
    "### Output (second) layer:\n",
    "\n",
    "$dA^{[2]} = \\nabla J_{A^{2}}(A^{[2]}, y) = -\\frac{y}{A^{[2]}}$ - for the softmax activation\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - y$\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m}(dZ^{[2]})^T\\left(A^{[1]}\\right)$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis = 0, keepdims = True) $\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[2]}:= W^{[2]} - \\eta dW^{[2]}$\n",
    "    \n",
    "$b^{[2]}:= b^{[2]} - \\eta db^{[2]}$\n",
    "\n",
    "### First layer:\n",
    "\n",
    "$dA^{[1]} = dZ^{[2]}W^{[2]}$\n",
    "\n",
    "$dZ^{[1]} = dA^{[1]}g^{[1]}\\_deriv(Z^{[1]})$\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}(dZ^{[1]})^T\\left(A^{[0]}\\right)$\n",
    "\n",
    "$db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis = 0, keepdims = True) $\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[1]}:= W^{[1]} - \\eta dW^{[1]}$\n",
    "    \n",
    "$b^{[1]}:= b^{[1]} - \\eta db^{[1]}$\n",
    "\n",
    "$A^{[0]} = X$.\n",
    "\n",
    "\n",
    "$g^{[1]}\\_deriv$ - derivative of activation function $g^{[1]}$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T13:10:27.557695Z",
     "start_time": "2019-11-21T13:10:27.547706Z"
    }
   },
   "source": [
    "## Generalization for L layers (L - 1 hidden layers)\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "In a for - loop over the layers, in which $l$ is the current layer:\n",
    "    \n",
    "$Z^{[l]} = A^{[l-1]}(W^{[l]})^T + b^{[l]}$\n",
    "\n",
    "$A^{[l]} = g^{[l]}(Z^{[l]})$\n",
    "\n",
    "$g^{[l]}$ - activation function in layer $l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T13:10:05.485502Z",
     "start_time": "2019-11-21T13:10:05.470495Z"
    }
   },
   "source": [
    "### Backward propagation\n",
    "\n",
    "For the output layer $L$, at which backpropagation starts:\n",
    "\n",
    "$dA^{[L]} = -\\frac{y}{A^{[L]}}$\n",
    "\n",
    "as the activation function of the output layer is the softmax and the number of neurons in the layer is $C$ - the number of classes considered.\n",
    "\n",
    "The back propagation is performed in a for - loop over the layers, starting with the output layer, down to the first layer, in which $l$ is the current layer:\n",
    "\n",
    "\n",
    "$dZ^{[l]} = dA^{[l]}g^{[l]}\\_deriv(Z^{[l]})$\n",
    "\n",
    "$dW^{[l]} = \\frac{1}{m}(dZ^{[l]})^T\\left(A^{[l - 1]}\\right)$\n",
    "\n",
    "$db^{[l]} = \\frac{1}{m}np.sum(dZ^{[l]}, axis = 0, keepdims = True) $\n",
    "\n",
    "$dA^{[l - 1]} = dZ^{[l]}W^{[l]}$\n",
    "\n",
    "\n",
    "__Weights updates:__\n",
    "    \n",
    "$W^{[l]}:= W^{[l]} - \\eta dW^{[l]}$\n",
    "    \n",
    "$b^{[l]}:= b^{[l]} - \\eta db^{[l]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:21:22.219238Z",
     "start_time": "2019-12-16T07:21:20.188041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.16.4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:21:22.484864Z",
     "start_time": "2019-12-16T07:21:22.250446Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './_semeion/semeion.data'\n",
    "df = pd.read_csv(path, sep=' ', header=None)\n",
    "all_values = df.values[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:21:24.000470Z",
     "start_time": "2019-12-16T07:21:23.984844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, 266)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:22:20.751128Z",
     "start_time": "2019-12-16T07:22:20.735622Z"
    }
   },
   "outputs": [],
   "source": [
    "#set up the input examples and the corresponding classes\n",
    "X_semeion = all_values[:, :-10]\n",
    "y_semeion = all_values[:, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:22:21.594889Z",
     "start_time": "2019-12-16T07:22:21.548126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n"
     ]
    }
   ],
   "source": [
    "#define the training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "m = X_semeion.shape[0]\n",
    "print(m)\n",
    "\n",
    "\n",
    "X_semeion_train, X_semeion_test, y_semeion_train, y_semeion_test = train_test_split(X_semeion, y_semeion,test_size = 0.66, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T07:22:24.048046Z",
     "start_time": "2019-12-16T07:22:24.016909Z"
    }
   },
   "outputs": [],
   "source": [
    "#initialization of the parameters\n",
    "np.random.seed(10) \n",
    "def initialization(dim_layers):\n",
    "    #dim_layers - array containing the dimensions of each layer\n",
    "    #dim_layers[0] - dimension of an input example\n",
    "    #dim_layers[L-1] - dimension of the output = number of classes\n",
    "    #L - the number layers\n",
    "    #parameters - dictionary for the parameters W and b\n",
    "    #the W[l] - parameters should be set as random values with normal distribution N(0, 1), multiplied with 0.01 \n",
    "    #the size of W[l] is (dim_layers[l] x dim_layers[l-1])\n",
    "    #the b[l] - parameters are initially arrays of 0 with dimensions (1 x dim_layers[l]) \n",
    "    #l - index of the layer\n",
    "    \n",
    "    L = len(dim_layers)\n",
    "    parameters ={}\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(dim_layers[l],dim_layers[l-1])*0.01#np.sqrt(1./dim_layers[l-1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((1, dim_layers[l]))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:26.807132Z",
     "start_time": "2019-12-10T16:54:26.775996Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:27.572768Z",
     "start_time": "2019-12-10T16:54:27.541630Z"
    }
   },
   "outputs": [],
   "source": [
    "def ReLu(z):\n",
    "    rez = z.copy()\n",
    "    rez[rez<0] = 0\n",
    "    return rez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:28.260278Z",
     "start_time": "2019-12-10T16:54:28.244652Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    rez = np.exp(z)\n",
    "    suma = np.sum(rez, axis = 1, keepdims = True)\n",
    "    \n",
    "    rez = rez/suma\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:28.932158Z",
     "start_time": "2019-12-10T16:54:28.916625Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    rez = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:29.604041Z",
     "start_time": "2019-12-10T16:54:29.588535Z"
    }
   },
   "outputs": [],
   "source": [
    "def ReLuBack(rlu):\n",
    "    grad = rlu.copy()\n",
    "    grad[grad>0] = 1\n",
    "    grad[grad<=0]= 0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:30.307175Z",
     "start_time": "2019-12-10T16:54:30.291669Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanhBack(tan):\n",
    "    grad = (1 - tan)**2\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:30.994684Z",
     "start_time": "2019-12-10T16:54:30.979129Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoidBack(sigm):\n",
    "    grad = sigm*(1-sigm)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:32.416579Z",
     "start_time": "2019-12-10T16:54:32.385439Z"
    }
   },
   "outputs": [],
   "source": [
    "#Estimation of the linear model Z in the current layer with calculation of the activation of Z\n",
    "\n",
    "def model_estimate(A_prev, W, b, activation):\n",
    "    #A_prev = activation of the precedent layer\n",
    "    #W, b - parameters for the current layer\n",
    "    #activation  - activation function for the current layer\n",
    "    #return value Z - linear model Z = A_prew@W.T + b\n",
    "    #return value A - activation of the current layer\n",
    "    \n",
    "    Z = A_prev@W.T + b    \n",
    "    \n",
    "    if activation ==\"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"ReLu\":\n",
    "        A = ReLu(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    else:\n",
    "        A = softmax(Z)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:27:40.612567Z",
     "start_time": "2019-12-10T17:27:40.596941Z"
    }
   },
   "outputs": [],
   "source": [
    "#One pass of the forwar propagation\n",
    "\n",
    "def forward_prop(X, y, parameters, activations, number_of_layers):\n",
    "    #number_of_layers - number of layers in the model\n",
    "    #cache - a dictionary containing the linear results and the activations in each layer,\n",
    "    #obtained during forward propagation\n",
    "    #X - input data of size (m x X.shape[1]), m = X.shape[0] = number of the training examples\n",
    "    #y - ideal output z size (m x C), C = number of classes\n",
    "    #parameters - current parameters of the NN = dictionary containing the values for the parameters in each layer\n",
    "    \n",
    "    #return value AL - activation of the last layer = current output of the NN, of size (m x C)\n",
    "    #return value cache - the cache containing the current linear models and activations on each layer\n",
    "    \n",
    "    #to obtain the parameters on layer l, use the values parameters[\"W\"+str(l)], parameters[\"b\"+str(l)]\n",
    "    \n",
    "    L = number_of_layers\n",
    "    cache = {}\n",
    "    A_prev = X\n",
    "    \n",
    "    cache[\"A0\"] = X \n",
    "    for l in range(1, L):\n",
    "        activation = activations[l]\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        Z, A = model_estimate(A_prev, W, b, activation)\n",
    "        cache[\"A\"+str(l)] = A\n",
    "        cache[\"Z\" +str(l)] = Z\n",
    "        A_prev = A\n",
    "        \n",
    "    AL = A\n",
    "    return AL, cache \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:34.682231Z",
     "start_time": "2019-12-10T16:54:34.666721Z"
    }
   },
   "outputs": [],
   "source": [
    "#error function = cross-entropy\n",
    "def error(X, y, AL):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    loss = np.sum(y*np.log(AL), axis = 1)\n",
    "    J = -1./m*np.sum(loss)\n",
    "        \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:35.541629Z",
     "start_time": "2019-12-10T16:54:35.510363Z"
    }
   },
   "outputs": [],
   "source": [
    "#derivative of the activation in the current layer, as function of the activation, for sigmoid, ReLu and tanh\n",
    "\n",
    "def deriv(activation, z):\n",
    "    if activation == \"sigmoid\":\n",
    "        rez = sigmoidBack(z)\n",
    "    elif activation == \"ReLu\":\n",
    "        rez = ReLuBack(z)\n",
    "    else:\n",
    "        rez = tanhBack(z)\n",
    "    \n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:36.338499Z",
     "start_time": "2019-12-10T16:54:36.322986Z"
    }
   },
   "outputs": [],
   "source": [
    "#function which calculates the derivative of the error with respect to the linear model Z in the current level,\n",
    "#as function of the derivative of the activation in the previous level, the activation in the current level\n",
    "# and the activation function\n",
    "\n",
    "def calculate_dz(dA, A, y, activation):\n",
    "    if activation == \"softmax\":\n",
    "        rez = A - y\n",
    "    else:\n",
    "        rez = dA*deriv(activation, A)\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:37.635389Z",
     "start_time": "2019-12-10T16:54:37.604253Z"
    }
   },
   "outputs": [],
   "source": [
    "#one iteration of back propagation\n",
    "def back_prop(y, cache, activations, parameters, number_of_layers, learning_rate):\n",
    "    #y - the ideal output\n",
    "    #cache - the cache obtained in the forward prop, containind the activations and the linear models in each layer\n",
    "    #activations - array containing the activation function names for each layer\n",
    "    #activations[0] = \" \". There is no activation on the input layer\n",
    "    #parameters - a dictionary contaning the actual values of the parameters W and b in each layer\n",
    "    #\n",
    "    \n",
    "    #return value - actual_parameters = actualized parameters W, b\n",
    "    \n",
    "    L = number_of_layers - 1 #index of the last layer\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    AL = cache[\"A\"+str(L)]\n",
    "    activation = activations[L]\n",
    "    \n",
    "    dA = -y/AL\n",
    "\n",
    "    actual_param = parameters.copy()\n",
    "    A = AL\n",
    "    \n",
    "    for l in range(L,0, -1):\n",
    "        dZ = calculate_dz(dA, A, y, activation)\n",
    "        A_prev = cache[\"A\"+str(l-1)]\n",
    "        \n",
    "        Z = cache[\"Z\"+str(l)]\n",
    "        \n",
    "        dW = 1./m*(dZ.T@A_prev)\n",
    "        db = 1./m*np.sum(dZ, axis = 0, keepdims = True)\n",
    "                        \n",
    "        activation = activations[l-1]\n",
    "        W = actual_param[\"W\" + str(l)]\n",
    "        \n",
    "        dA = dZ@W\n",
    "        A = A_prev\n",
    "       \n",
    "            \n",
    "        actual_param[\"W\" + str(l)] = actual_param[\"W\" + str(l)] - learning_rate*dW\n",
    "        actual_param[\"b\" + str(l)] = actual_param[\"b\" + str(l)] - learning_rate*db\n",
    "        \n",
    "        \n",
    "    return actual_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:38.463524Z",
     "start_time": "2019-12-10T16:54:38.447975Z"
    }
   },
   "outputs": [],
   "source": [
    "#one iteration thrugh the NN: forward propagation - error calculation - back propagation\n",
    "\n",
    "def one_iteration(X, y, parameters, number_of_layers, activations, learning_rate = 0.01):\n",
    "    #X - training set - batch or minibatch\n",
    "    #y - ideal output for the trainig set\n",
    "    #parameters - the actual parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #learning_rate - learning rate for the update of the parameters\n",
    "    #AL - activation on the last layer - a numpy array of dimensions (m x C), C - number of classes, m - X.shape[0]\n",
    "    #cache - a dictionary containing the activations and linear models on each layer, obtained during forward prop\n",
    "    \n",
    "    #returned value J - error of the current NN - calculated with cross-entropy\n",
    "    #returned value parameters - actualized parameters W, b for each layer, calculated during back prop\n",
    "    \n",
    "    AL, cache = forward_prop(X, y, parameters, activations, number_of_layers)\n",
    "    J = error(X,y,AL)\n",
    "       \n",
    "    parameters = back_prop(y, cache, activations, parameters, number_of_layers, learning_rate)\n",
    "    \n",
    "    return J, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:39.276033Z",
     "start_time": "2019-12-10T16:54:39.260523Z"
    }
   },
   "outputs": [],
   "source": [
    "#function which iterates through the NN, to minimize the model\n",
    "#output of the function  - the error list and the estimated weights for the model\n",
    "\n",
    "def run_model(X, y, parameters, number_of_layers, activations, learning_rate = 0.1, number_of_iteration = 1000):\n",
    "    #X - training set - batch or minibatch\n",
    "    #y - ideal output for the trainig set\n",
    "    #parameters - the actual parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #learning_rate - learning rate for the update of the parameters    \n",
    "    \n",
    "    #return value w_err - vector of errors at each iteration\n",
    "    #return value param  - estimated parameters\n",
    "    \n",
    "    w_err=[]\n",
    "    param = parameters.copy()\n",
    "       \n",
    "     \n",
    "    for nr in range(number_of_iteration):\n",
    "        J, param = one_iteration(X, y, param, number_of_layers, activations, learning_rate)\n",
    "        w_err.append(J)\n",
    "            \n",
    "    return w_err, param    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:40.229171Z",
     "start_time": "2019-12-10T16:54:40.213618Z"
    }
   },
   "outputs": [],
   "source": [
    "#dim_layers - vector containing the size of each layer. Layer 0 - dimension of a training example\n",
    "#activations - vector with the activations on each layer. The input layer has no activation function (identity)\n",
    "\n",
    "input_size = X_semeion_train.shape[1]\n",
    "output_size = y_semeion_train.shape[1]\n",
    "\n",
    "dim_layers = np.array([input_size, 4, output_size ]) #a NN with one hidden layer and 4 neurons\n",
    "activations = np.array([\" \",\"ReLu\", \"softmax\"]) #on the hidden layer we have ReLu activation function\n",
    "\n",
    "parameters = initialization(dim_layers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:56:28.496108Z",
     "start_time": "2019-12-10T16:56:26.636825Z"
    }
   },
   "outputs": [],
   "source": [
    "w_err, param = run_model(X_semeion_train, y_semeion_train, parameters, len(dim_layers), activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:56:29.589874Z",
     "start_time": "2019-12-10T16:56:29.558697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08451932118493646\n",
      "{'W1': array([[-0.30644537, -0.34755865, -0.28011951, ...,  0.20789721,\n",
      "        -0.16472381, -0.15058187],\n",
      "       [ 0.4423708 ,  0.34919236,  0.07471693, ..., -0.18608843,\n",
      "        -0.0957368 ,  0.08523051],\n",
      "       [-0.37118564, -0.24694706, -0.15715863, ...,  0.24548307,\n",
      "         0.44939891,  0.22077357],\n",
      "       [ 0.23694616,  0.25430873,  0.29606006, ..., -0.1591125 ,\n",
      "        -0.31102993, -0.3437852 ]]), 'b1': array([[-0.11759183,  0.28394951,  0.11946653, -0.02952789]]), 'W2': array([[ 1.44623574, -0.99976127, -1.57724834,  0.51355372],\n",
      "       [-1.47246022,  1.33966049,  0.80021006, -1.41975513],\n",
      "       [-0.42282277, -1.05344537,  1.92526753, -1.19544275],\n",
      "       [-1.98175607, -0.45481135,  0.63767284,  1.45515344],\n",
      "       [ 1.22464189,  0.67091983, -1.18551619, -0.82887903],\n",
      "       [ 0.39632375, -1.1415739 , -0.27290081,  1.30813559],\n",
      "       [ 1.39417838, -1.00517003,  0.26866404, -0.97707772],\n",
      "       [-0.55676024,  2.01346137, -0.9803564 , -0.06466272],\n",
      "       [ 0.3705493 ,  0.18313418,  0.60688382, -0.08573517],\n",
      "       [-0.40224616,  0.44083973, -0.27066019,  1.28109197]]), 'b2': array([[-0.19271409,  0.55985025,  0.26038365,  0.11743126,  0.32060053,\n",
      "         0.24220348, -0.38432421,  0.21529589, -0.81704222, -0.32168454]])}\n"
     ]
    }
   ],
   "source": [
    "print(w_err[len(w_err)-1])\n",
    "print(param)\n",
    "#print(w_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:54:51.104319Z",
     "start_time": "2019-12-10T16:54:50.776262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(w_err))),w_err)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:38.153933Z",
     "start_time": "2019-12-10T16:58:38.138421Z"
    }
   },
   "outputs": [],
   "source": [
    "#predictions for input values X\n",
    "def prediction(X, y, parameters, activations, number_of_layers):\n",
    "    #X - input data \n",
    "    #y - ideal output for the input data\n",
    "    #parameters - estimated parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    #return value pred - array containing for each example the position of the maximum probability = corresponding to the class\n",
    "    \n",
    "    AL, _ = forward_prop(X, y, parameters, activations,  number_of_layers)\n",
    "    \n",
    "    pred = np.argmax(AL, axis=1)\n",
    "    \n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:39.107068Z",
     "start_time": "2019-12-10T16:58:39.091558Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_prediction(X, y, parameters, activations, number_of_layers):\n",
    "    #X - input data \n",
    "    #y - ideal output for the input data\n",
    "    #parameters - estimated parameters W, b for each layer\n",
    "    #number_of_layers - number of layers of the NN\n",
    "    #activations - array containing the activation functions for each layer\n",
    "    \n",
    "    #return value - percent - percentage of correct predictions\n",
    "    nr = 0\n",
    "   \n",
    "    pred = prediction(X, y, parameters, activations, number_of_layers)\n",
    "    classes = np.argmax(y, axis=1)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        if pred[i] == classes[i]:\n",
    "            nr +=1\n",
    "   \n",
    "    percent = nr/X.shape[0]*100\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:40.013328Z",
     "start_time": "2019-12-10T16:58:39.982149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.44547134935306\n"
     ]
    }
   ],
   "source": [
    "number_of_layers = len(dim_layers)\n",
    "rez = test_prediction(X_semeion_train, y_semeion_train, param, activations, number_of_layers)\n",
    "print(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:43.216495Z",
     "start_time": "2019-12-10T16:58:43.185361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(530, 10)\n",
      "82.64150943396227\n"
     ]
    }
   ],
   "source": [
    "m = X_semeion.shape[0]\n",
    "X_semeion_test = X_semeion[2*m//3+1:,:]\n",
    "y_semeion_test = y_semeion[2*m//3+1:,:]\n",
    "print(y_semeion_test.shape)\n",
    "rez2 = test_prediction(X_semeion_test, y_semeion_test, param, activations, number_of_layers)\n",
    "print(rez2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
