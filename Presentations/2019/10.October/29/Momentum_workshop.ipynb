{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum method\n",
    "Presented during ML reading group, 2019-10-29.\n",
    "\n",
    "Author: Lucian Sasu, lmsasu@unitbv.ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.537230Z",
     "start_time": "2019-10-28T19:06:46.949802Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "print(f'Numpy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stochastic gradient descent, the weight are modified as follows:\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\Delta \\mathbf{w}_{t+1}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\Delta \\mathbf{w}_{t+1} = -\\eta \\nabla_{\\mathbf{w}}J(\\mathbf{w}_{t})\n",
    "$$\n",
    "The momentum method usually accelerates the training process. The momentum method says that the modification of current weights is based not only on the gradient, but also on the previous weights' modification:\n",
    "$$\n",
    "\\Delta \\mathbf{w}_{t+1} = \\gamma \\Delta \\mathbf{w}_{t} - \\eta \\nabla_{\\mathbf{w}}J(\\mathbf{w}_{t})\n",
    "$$\n",
    "and thus the modification of weights becomes:\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\Delta \\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\gamma \\Delta \\mathbf{w}_{t} - \\eta \\nabla_{\\mathbf{w}}J(\\mathbf{w}_{t})\n",
    "$$\n",
    "\n",
    "In this way, the search is influenced by the previous direction. It tends to preserve the direction of search and prevents oscillations. In most libraries, $\\gamma$ defaults to 0.9.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate input 2d data, $\\mathbf{x} = (x_1, x_2) \\in [-scale1, scale1] \\times [-scale2, scale2]$, independently and uniformly distributed. The ground truth values associated with a pair $(x, y)$ is $y=a \\cdot x_1 + b \\cdot x_2$ (note lack of intercept), to which we add random noise from $\\mathcal{N}(0, 1)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T18:39:02.099213Z",
     "start_time": "2019-10-31T18:39:02.093223Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10) # for reproducibility\n",
    "m = 100\n",
    "scale1, scale2 = 1, 1 # play with these... \n",
    "\n",
    "a, b = 3, 7\n",
    "\n",
    "def gen_data(m, scale1, scale2, a, b, add_noise=True):\n",
    "    # produce: X, a 2d tensor with n lines and 2 columns\n",
    "    # and X[:, 0] uniformly distributed in [-scale1, scale1], X[:, 1] in [-scale2, scale2]\n",
    "    # let y be a*X[:, 0] + b*X[:, 1] + epsilon, with epsilon ~ N(0, 1); y is a column vector with n elements\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.562163Z",
     "start_time": "2019-10-28T19:06:47.556179Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = gen_data(n_data, scale1, scale2, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d plot $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.896269Z",
     "start_time": "2019-10-28T19:06:47.565155Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "# fig, axes = plt.subplots()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], y, marker='o')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define error function, gradient, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.905246Z",
     "start_time": "2019-10-28T19:06:47.899261Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_estimate(X, w):\n",
    "    '''Computes the linear regression estimation on the dataset X, using coefficients w\n",
    "    :param X: a 2d tensor with n lines and 2 columns\n",
    "    :param w: a 1d tensor with 2 coefficients (no intercept)\n",
    "    :return: a 1d tensor witn n lines, y_hat = X[:, 0] * w[0] + X[:, 1] * w[1]\n",
    "    '''\n",
    "    # vectorized implementation of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.916216Z",
     "start_time": "2019-10-28T19:06:47.909235Z"
    }
   },
   "outputs": [],
   "source": [
    "def J(X, y, w):\n",
    "    \"\"\"Computes the mean squared error of the model. See the picture below for formula.\n",
    "    :param X: input values, of shape n x 2\n",
    "    :param y: ground truth, column vector with n values\n",
    "    :param w: column with 2 coefficients for the linear form \n",
    "    :return: a scalar value >= 0\n",
    "    \"\"\"\n",
    "    # vectorized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.928184Z",
     "start_time": "2019-10-28T19:06:47.920206Z"
    }
   },
   "outputs": [],
   "source": [
    "J(X, y, w=np.array([3, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.934171Z",
     "start_time": "2019-10-28T19:06:47.930178Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: contour plots of error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cheatsheet](https://medium.com/ml-ai-study-group/vectorized-implementation-of-cost-functions-and-gradient-vectors-linear-regression-and-logistic-31c17bca9181)\n",
    "\n",
    "![Cheatsheet](https://miro.medium.com/max/1408/1*PZ3TTZZIT1wlqyt05TpZBg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.943145Z",
     "start_time": "2019-10-28T19:06:47.937160Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, w):\n",
    "    '''Commputes the gradients to be used for gradient descent. \n",
    "    :param X: 2d tensor with training data\n",
    "    :param y: 1d tensor with y.shape[0] == W.shape[0]\n",
    "    :param w: 1d tensor with current values of the coefficients\n",
    "    :return: gradients to be used for dradgradient descent. See picture above\n",
    "    '''\n",
    "    n = len(y)\n",
    "    w = w.reshape(-1, 1)\n",
    "    return ## implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.953117Z",
     "start_time": "2019-10-28T19:06:47.946137Z"
    }
   },
   "outputs": [],
   "source": [
    "gradient(X, y, w=np.array([3, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.968077Z",
     "start_time": "2019-10-28T19:06:47.956111Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd_no_momentum(X, y, w_init, eta=1e-1):\n",
    "    '''Iterates with gradient descent. algorithm\n",
    "    :param X: 2d tensor with data\n",
    "    :param y: 1d tensor, ground truth \n",
    "    :param w_init: 1d tensor with the 2 initil coefficients\n",
    "    :param eta: the learning rate hyperparameter\n",
    "    :return: the list of succesive errors and the found w* vector \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:47.993010Z",
     "start_time": "2019-10-28T19:06:47.972068Z"
    }
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0, 0])\n",
    "errors, w_best = gd_no_momentum(X, y, w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.006974Z",
     "start_time": "2019-10-28T19:06:47.996002Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'How many iterations were made: {len(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.015949Z",
     "start_time": "2019-10-28T19:06:48.009965Z"
    }
   },
   "outputs": [],
   "source": [
    "w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.248327Z",
     "start_time": "2019-10-28T19:06:48.018943Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(errors))), errors)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')\n",
    "axes.set_title('Optimization without momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.255310Z",
     "start_time": "2019-10-28T19:06:48.250323Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show evolution of w on a 2d countour plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.271267Z",
     "start_time": "2019-10-28T19:06:48.258301Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd_with_momentum(X, y, w_init, eta=1e-1, gamma = 0.9):\n",
    "    \"\"\"Applies grdient descent with momentum coefficient\n",
    "    :params: as in gd_no_momentum\n",
    "    :param gamma: momentum coefficient\n",
    "    :return: the list of succesive errors and the found w* vector \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.289224Z",
     "start_time": "2019-10-28T19:06:48.278252Z"
    }
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0, 0])\n",
    "errors_momentum, w_best = gd_with_momentum(X, y, w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.299192Z",
     "start_time": "2019-10-28T19:06:48.293208Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'How many iterations were made: {len(errors_momentum)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.309165Z",
     "start_time": "2019-10-28T19:06:48.302184Z"
    }
   },
   "outputs": [],
   "source": [
    "w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.542540Z",
     "start_time": "2019-10-28T19:06:48.312157Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.plot(list(range(len(errors_momentum))), errors_momentum)\n",
    "axes.set_xlabel('Epochs')\n",
    "axes.set_ylabel('Error')\n",
    "axes.set_title('Optimization with momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.549523Z",
     "start_time": "2019-10-28T19:06:48.544535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: if one plays with scale1, scale2 from above - e.g. scale1=10, scale2=1 - then the results are very interesting...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T19:06:48.560495Z",
     "start_time": "2019-10-28T19:06:48.552515Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show evolution of w on a 2d countour plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AdaGrad paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) comes with the idea of using different learning rates for each feature. Hence, instead of:\n",
    "![eq1](./images/eq1.png)\n",
    "\n",
    "AdaGrad comes with:\n",
    "![eq2](./images/eq2.png)\n",
    "\n",
    "where $g_{t}^{(j)}$ is the gradient of error function, i.e. \n",
    "![eq3](./images/eq3.png)\n",
    "\n",
    "We will use vector notations:\n",
    "![eq4](./images/eq4.png)\n",
    "\n",
    "AdaGrad specifies the update as:\n",
    "![eq5](./images/eq5.png)\n",
    "where:\n",
    "* $\\eta$ is the initial learning rate (hyperparameter)\n",
    "* $n$ is the number of items in (mini)batch\n",
    "* $G_t = \\sum\\limits_{\\tau=1}^t \\mathbf{g}_\\tau \\mathbf{g}_\\tau^T$\n",
    "* $diag(A)$ is the diagonal form of the square matrix $A$\n",
    "* $\\varepsilon > 0$ is used to avoid division by 0\n",
    "* $I$ is the unit matrix of size $m$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more detailed form, the update of the weights through AdaDelta is done by:\n",
    "![eq6](./images/eq6.png)\n",
    "which simplifies to:\n",
    "![eq7](./images/eq7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define error function, gradient, inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply AdaGrad and report resulting $\\eta$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.712px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
